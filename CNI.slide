Introduction to CNI
Glue to network plugins

Deepak Gunjetti
Software Architect @ Andcloud
deepak@andcloud.io
@dgunjetti

* How are containers made

- containers are made of 

Namespaces
  - Process
  - Network
  - Mount 
  - UTS
  - IPC
  - User

CGroups
  - CPU
  - Memory

Layered Filesystem
- AUSF


* Linux Network Namespace

- Create container network isolation instances.

- Own networking stack, routing table, Virtual interfaces, L2 isolation.

- Tool used to work with network ns - iproute2 

- Network ns are stored in /var/run/netns

- there are two types of network namespaces

   - root namespace (ip link)

   - non-root namespace (ip netns .. ip link)

```
ip netns add namespace1
ip netns
ls -l /var/run/netns
```

* Kubernetes Networking

          Linux Host 
  Pod1 Namespace     Pod2 Namespace
  Container1          Container3
  Container2          Container4
  eth0 10.0.0.5       eth0 10.0.0.6

  vethxx              vethxx
  docker0 - 10.0.0.1

  eth0 <-> internet

* container networking interface (CNI)

- CNI concerns itself only creating network connectivity for containers when containers are created, and removing network connectivity when containers are deleted.

- It consists of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins.

- CNI acts as glue between orchestrator (runtime) and network implementation (plugin)

- CNI Plugin implement CNI Spec.

- Orchestrator calls CNI plugin to program the network namespace.

- CNI plugins have standard interface, so orchestrator is abstracted from which plugin is actually used underneth.

- Orchestrator and Plugin use CNI Library.

* CNI

https://github.com/containernetworking

Git repo has two parts.

- Plugin

- CNI
    - Spec
    - API/Library

* Spec

- specification defines the basic flow and configuration format of network operations.
how plugin is given data by runtime and how status is reported back to runtime.


* configuration format

- JSON based configuration.

- It can contain standard key/value pair and plugin specific key/value pair.

- Configuration is fed to plugin on stdin on each operation.

- Stored on disk or by the runtime.

- configuration is fed to each plugin operation.

* configuration format..

  {
    "cniVersion":
    "name:
    "type": ptp
    "ipam": {
      "type": "host-local",
      "subnet": "10.0.0.0/24"
    }
  }

* Execution flow

- Basic commands: ADD DEL and VERSION

- ADD - add container to given network

- DEL - remove container from given network

- VERSION - return version of plugin.

* Execution flow

  CNI_COMMAND=VERSION ./ptp < conf

shows the version supported.

  ip netns add demo

  CNI_COMMAND=ADD CNI_NETNS=/var/run/netns/demo CNI_IFNAME=demoeth0 CNI_PATH=/cni ./ptp < conf

- demoeth0 interface is created inside the network namespace demo with IP address 10.0.0.10 and adds default route pointing to 10.0.0.1

- vethxx interface is created on host root namespace with ipaddress 10.0.0.1 and adds route to 10.0.0.10 point to vethxx

* Execution flow

  CNI_COMMAND=DEL CNI_NETNS=/var/run/netns/demo CNI_IFNAME=demoeth0 CNI_PATH=/cni ./ptp < conf

- Interfaces and routes are deleted in container and host root namespace.

* Plugins

- Plugins are executables

- Spawned(fork/exec) by runtime when network operation is desired, when container is created/deleted.

- Operations are fed by environment variables and configuration via stdin

- Plugins are also fed container-specific data via stdin - name of container, namespace, bandwidth requirements, QoS, portmapping - map container internal port to that of host

- Plugins report structured (JSON) result via stdout back to runtime - IP address, interfaces created, routes and DNS info

* Plugins..

- Some CNI network plugins, maintained by the containernetworking team.

Main - interface creating, L2 connectivity on host.
- bridge, ipvlan, macvlan, ptp, host-device, windows

IPAM - ip address allocation
- DHCP, host-local, static

Meta - other plugins,  bandwidth or qos guarantees
- flannel, tuning, portmap, bandwidth

* 3rd Party plugins

- Project calico

- Flannel

- Canal


* Kubernetes with Project Calico network plugin

create a container in kubernetes.

- API server calls Kubelet to create pod.

- Kubelet will create network namespace for pod - pause container or infrastructure container.

- Kubelet will call CNI with ADD command using CNI API library. kubelet - calico CNI

- Calico CNI will query API Server for labels and annotations.

- Calico CNI calls Calico IPAM to get IP address.

- Calico IPAM queries centralized datastore for IP address, returns IP address to Calico CNI.

* Project Calico..

- Calico CNI proceeds to network the Pod.

- Create veth pair, eth0 on pod side, calixxx (some hash) on host side.

- Add ip address to eth0, add default route.

- Configure sysctl on host side.

- Add route to container on host.

- Calico CNI reports to kubelet that container is networked.

- Kubelet reports pod IP address to API server.

* Project Calico...

  apiVersion: v1
  kind: Node
  -
  status:
    addresses: 
      - type: InternalIP
        addresses: 10.0.0.43
  spec:
    podCIDR: 10.50.0.0/24 // block of ip address that node is going to use for assign pod's ip.


* Project Calico...


  apiversion: v1
  kind: Pod
  -
  status: 
    hostIP: 10.0.0.43
    podIP: 10.50.0.20

- podCIDR is not in host subnet.

- Node has single network interface

- Node is given pod cidr

- Each pod has single network interface always called eth0, has single ip. 

* Summary

- CNI plugin is responsible for creation of interfaces inside network namespaces (eth0 - veth), IP address allocation from PodCIDR, adding routes to make Pod reachable by the whole cluster.

- CNI Plugin provides

    - Connectivity

    - Reachability

* Why do we need plugins

- Because of diverse space and requirements.

- Connectivity might depend on network interface hardware, Isolation technology (namespace or VM).

- Reachability depends on networking hardware inside cloud, which is not accessible.

- How do you talk to networking hardware, BGP, OSPF. 

- Plugin system abstracts all these things.

- Kubernetes does not match the traditional networking environments.

* Plugins.. 

- Traditional environments

    - Nodes live long, have predictable lifecycle.

    - Addressing is topoloby aware (subnet for datacenter, rack)

    - one interface = one IP

- Kubernetes environment

    - IP address allocation is topology unware

    - Allocates 100+ IPs per node, each physical port on network needs to handle much more addressing.

    - Lifetimes are short, churn is high ( address is constantly changing, Pods are coming and going)

* How do CNI plugins work with Kubernetes

- CNI plugins have two components

    - CNI binary (calico CNI) that configures the Pod's interfaces (connectivity)

    - A daemon that manages routing (Reachability)

* Connectivity

- Create a 'veth' pair (point to point virtual tunnel)

- Move one end of the pair into the container's namespace.

- Configure an ip and route in the container's namespace.

- Packets leaving the container are simply routed through the host's IP stack. They are usually masqueraded.

https://lwn.net/Articles/531114/

* Reachability

- Goal - make every podCIDR which is assigned dynamically reachable from every node.

- Announce dynamic routes to some peers

- CNI Daemon that runs on every host, programs the network with learned routes.

* Reachability..

- Programming the podCIDR can take any forms.

    - overlay networks - programe every nodes route table, not routers. (VXLAN, IPIP) 
    routers need not know podCIDR

    - Routing protocol - OSPF/BGP 
        
        - podCIDR are real address in the network, reachable not only internal to 
          cluster but also by external to cluster.

        - Need to have router to talk to, routers need to have enough memory and scale 
        to accept the churn kubernetes creates.

- kubelet is configured with --cni-conf-dir which is scaned every 5 seconds and whichever one has the lowest ordered filenames.

- Write only one CNI configuration file to disk.


* Plugin chaining

  {
      "cniVersion":
      "name":
      "plugins": [
          { "type": "ptp" },
          { "type": "iptables-allow" },
          { "type": "tuning" },
          {   "sysctl": {
                  "net.core.somaxconn": "9001"
              }
          }
      ]
  }


- multiple plugins are executed in a row for a container.

- result of previous one is given to next plugin.

* Self hosting

- Deploy plugins using kubernetes.

- A manifest for CNI plugin should

    - Set up routing daemon

    - Copy the plugin binaries to the host

    - Install the configuration file

Installing the configuration file tells the host that the network is up. Do at last.

- Need to run daemon in host network.









